# Data Quality Guardrails — Technical Specifications

## Goal
Build a minimal local web app + API that uses a LangGraph multi-agent pipeline to profile a dataset, detect data quality issues/drift, validate schema, and recommend fixes. The UI shows a simple upload/preview, run pipeline, and renders a report.

## Scope (MVP)
- Local-only execution (no cloud services required).
- Upload CSV from UI or API.
- Run LangGraph pipeline with 4 agents:
  1) **Profiler**: column stats, missingness, data types, ranges, uniques.
  2) **Schema Validator**: infer schema + detect violations.
  3) **Drift Detector**: compare against optional baseline dataset.
  4) **Fix Recommender**: suggest cleaning steps (impute, cast, outlier handling).
- Generate a single JSON report + human-readable summary.
- Minimal UI: upload, run, view results.

## Non-Goals (for now)
- No cloud deployment.
- No authentication.
- No real-time streaming.
- No large dataset optimization beyond reasonable CSV sizes.

## Tech Stack
- **Backend**: Python (FastAPI)
- **Agent Orchestration**: LangGraph + LangChain
- **LLM**: OpenAI API (key via env var)
- **Data**: Pandas + NumPy
- **Validation/Profiling**: Pandas profiling logic (custom) + optional Great Expectations (future)
- **Frontend**: Minimal React (Vite) or simple HTML/JS if time constrained
- **API Docs**: FastAPI autogenerated docs

## API Design
- `POST /api/analyze` — upload CSV, optional baseline CSV
  - Request: `multipart/form-data`
  - Response: JSON report
- `GET /api/health` — health check

## UI Requirements
- Single page with:
  - File picker for dataset
  - Optional baseline file
  - Run button
  - Results panel (JSON + summary)

## Project Structure (proposed)
- `backend/`
  - `main.py` (FastAPI app)
  - `pipeline/` (LangGraph nodes)
  - `schemas.py` (report models)
  - `utils/` (CSV parsing, stats helpers)
- `frontend/`
  - Vite React app (or plain HTML)

## LangGraph Pipeline
- Input: dataset (and optional baseline)
- Nodes:
  - `profile_node`
  - `schema_node`
  - `drift_node`
  - `fix_node`
- Output: `report` (dict) with sections: `profile`, `schema`, `drift`, `recommendations`, `summary`

## Success Criteria
- User can upload a CSV and receive a structured report.
- Pipeline runs end-to-end locally.
- UI displays results clearly.

## Future Enhancements
- Great Expectations integration
- Persistent reports (SQLite)
- Auth
- Deployment to Vercel + Supabase
